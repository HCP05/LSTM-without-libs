{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN - LSTM without any Libraries.......\n",
    "![title](RNN-LSTM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RecurrentNeuralNetwork:\n",
    "    \n",
    "    def __init__ (self, input, output, recurrences, expected_output, learning_rate):\n",
    "        #initial input \n",
    "        self.x = np.zeros(input)\n",
    "        #input size \n",
    "        self.input = input\n",
    "        #expected output \n",
    "        self.y = np.zeros(output)\n",
    "        #output size\n",
    "        self.output = output\n",
    "        #weight matrix \n",
    "        self.w = np.random.random((output, output))\n",
    "        #matrix used in RMSprop in order to decay the learning rate\n",
    "        self.G = np.zeros_like(self.w)\n",
    "        #length of the recurrent network\n",
    "        self.recurrences = recurrences\n",
    "        #learning rate \n",
    "        self.learning_rate = learning_rate\n",
    "        #array for storing inputs\n",
    "        self.ia = np.zeros((recurrences+1,input))\n",
    "        #array for storing cell states\n",
    "        self.ca = np.zeros((recurrences+1,output))\n",
    "        #array for storing outputs\n",
    "        self.oa = np.zeros((recurrences+1,output))\n",
    "        #array for storing hidden states\n",
    "        self.ha = np.zeros((recurrences+1,output))\n",
    "        #forget gate \n",
    "        self.af = np.zeros((recurrences+1,output))\n",
    "        #input gate\n",
    "        self.ai = np.zeros((recurrences+1,output))\n",
    "        #cell state\n",
    "        self.ac = np.zeros((recurrences+1,output))\n",
    "        #output gate\n",
    "        self.ao = np.zeros((recurrences+1,output))\n",
    "        #array of expected output values\n",
    "        self.expected_output = np.vstack((np.zeros(expected_output.shape[0]), expected_output.T))\n",
    "        #declare LSTM cell \n",
    "        self.LSTM = LSTM(input, output, recurrences, learning_rate)\n",
    "    \n",
    "    #sigmoid activation function\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    #derivative of sigmoid \n",
    "    def dsigmoid(self, x):\n",
    "        return self.sigmoid(x) * (1 - self.sigmoid(x))    \n",
    "    \n",
    "    #Forward Propagation\n",
    "    def forwardProp(self):\n",
    "        for i in range(1, self.recurrences+1):\n",
    "            self.LSTM.x = np.hstack((self.ha[i-1], self.x))\n",
    "            cs, hs, f,inp, c, o = self.LSTM.forwardProp()\n",
    "            #store cell state from the forward propagation\n",
    "            self.ca[i] = cs #cell state\n",
    "            self.ha[i] = hs #hidden state\n",
    "            self.af[i] = f #forget state\n",
    "            self.ai[i] = inp #inpute gate\n",
    "            self.ac[i] = c #cell state\n",
    "            self.ao[i] = o #output gate\n",
    "            self.oa[i] = self.sigmoid(np.dot(self.w, hs)) #activate the weight*input\n",
    "            # print(f\"{self.expected_output} at iteration {i}\")\n",
    "            self.x = self.expected_output\n",
    "        return self.oa\n",
    "   \n",
    "    # Back propagation\n",
    "    def backProp(self):\n",
    "        totalError = 0\n",
    "        #cell state\n",
    "        dfcs = np.zeros(self.output)\n",
    "        #hidden state,\n",
    "        dfhs = np.zeros(self.output)\n",
    "        #weight matrix\n",
    "        tu = np.zeros((self.output,self.output))\n",
    "        #forget gate\n",
    "        tfu = np.zeros((self.output, self.input+self.output))\n",
    "        #input gate\n",
    "        tiu = np.zeros((self.output, self.input+self.output))\n",
    "        #cell unit\n",
    "        tcu = np.zeros((self.output, self.input+self.output))\n",
    "        #output gate\n",
    "        tou = np.zeros((self.output, self.input+self.output))\n",
    "        for i in range(self.recurrences, -1, -1):\n",
    "            error = self.oa[i] - self.expected_output[i]\n",
    "            tu += np.dot(np.atleast_2d(error * self.dsigmoid(self.oa[i])), np.atleast_2d(self.ha[i]).T)\n",
    "            error = np.dot(error, self.w)\n",
    "            self.LSTM.x = np.hstack((self.ha[i-1], self.ia[i]))\n",
    "            self.LSTM.cs = self.ca[i]\n",
    "            fu, iu, cu, ou, dfcs, dfhs = self.LSTM.backProp(error, self.ca[i-1], self.af[i], self.ai[i], self.ac[i], self.ao[i], dfcs, dfhs)\n",
    "            totalError += np.sum(error)\n",
    "            #forget gate\n",
    "            tfu += fu\n",
    "            #input gate\n",
    "            tiu += iu\n",
    "            #cell state\n",
    "            tcu += cu\n",
    "            #output gate\n",
    "            tou += ou   \n",
    "        self.LSTM.update(tfu/self.recurrences, tiu/self.recurrences, tcu/self.recurrences, tou/self.recurrences)  \n",
    "        self.update(tu/self.recurrences)\n",
    "        return totalError\n",
    "    \n",
    "    def update(self, u):\n",
    "        self.G = 0.95 * self.G + 0.1 * u**2  \n",
    "        self.w -= self.learning_rate/np.sqrt(self.G + 1e-8) * u\n",
    "        return\n",
    "    \n",
    "    def sample(self):\n",
    "        for i in range(1, self.recurrences+1):\n",
    "            self.LSTM.x = np.hstack((self.ha[i-1], self.x))\n",
    "            cs, hs, f, inp, c, o = self.LSTM.forwardProp()\n",
    "            maxI = np.argmax(self.x)\n",
    "            self.x = np.zeros_like(self.x)\n",
    "            self.x[maxI] = 1\n",
    "            self.ia[i] = self.x \n",
    "            #store cell states\n",
    "            self.ca[i] = cs\n",
    "            #store hidden state\n",
    "            self.ha[i] = hs\n",
    "            #forget gate\n",
    "            self.af[i] = f\n",
    "            #input gate\n",
    "            self.ai[i] = inp\n",
    "            #cell state\n",
    "            self.ac[i] = c\n",
    "            #output gate\n",
    "            self.ao[i] = o\n",
    "            self.oa[i] = self.sigmoid(np.dot(self.w, hs))\n",
    "            maxI = np.argmax(self.oa[i])\n",
    "            newX = np.zeros_like(self.x)\n",
    "            newX[maxI] = 1\n",
    "            self.x = newX\n",
    "           \n",
    "        return self.oa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long short-term memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    # LSTM cell (input, output, amount of recurrence, learning rate)\n",
    "    def __init__ (self, input, output, recurrences, learning_rate):\n",
    "        #input size\n",
    "        self.x = np.zeros(input+output)\n",
    "        #input size\n",
    "        self.input = input + output\n",
    "        #output \n",
    "        self.y = np.zeros(output)\n",
    "        #output size\n",
    "        self.output = output\n",
    "        #cell state intialized as size of prediction\n",
    "        self.cs = np.zeros(output)\n",
    "        #how often to perform recurrence\n",
    "        self.recurrences = recurrences\n",
    "        #balance the rate of training (learning rate)\n",
    "        self.learning_rate = learning_rate\n",
    "        #init weight matrices for our gates\n",
    "        #forget gate\n",
    "        self.f = np.random.random((output, input+output))\n",
    "        #input gate\n",
    "        self.i = np.random.random((output, input+output))\n",
    "        #cell state\n",
    "        self.c = np.random.random((output, input+output))\n",
    "        #output gate\n",
    "        self.o = np.random.random((output, input+output))\n",
    "        #forget gate gradient\n",
    "        self.Gf = np.zeros_like(self.f)\n",
    "        #input gate gradient\n",
    "        self.Gi = np.zeros_like(self.i)\n",
    "        #cell state gradient\n",
    "        self.Gc = np.zeros_like(self.c)\n",
    "        #output gate gradient\n",
    "        self.Go = np.zeros_like(self.o)\n",
    "    \n",
    "   \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def dsigmoid(self, x):\n",
    "        return self.sigmoid(x) * (1 - self.sigmoid(x))\n",
    "    \n",
    "    \n",
    "    def tangent(self, x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    \n",
    "    def dtangent(self, x):\n",
    "        return 1 - np.tanh(x)**2\n",
    "  \n",
    "    def forwardProp(self):\n",
    "        f = self.sigmoid(np.dot(self.f, self.x))\n",
    "        self.cs *= f\n",
    "        i = self.sigmoid(np.dot(self.i, self.x))\n",
    "        c = self.tangent(np.dot(self.c, self.x))\n",
    "        self.cs += i * c\n",
    "        o = self.sigmoid(np.dot(self.o, self.x))\n",
    "        self.y = o * self.tangent(self.cs)\n",
    "        return self.cs, self.y, f, i, c, o\n",
    "    \n",
    "   \n",
    "    def backProp(self, e, pcs, f, i, c, o, dfcs, dfhs):\n",
    "        \n",
    "        e = np.clip(e + dfhs, -6, 6)\n",
    "        \n",
    "        do = self.tangent(self.cs) * e\n",
    "        \n",
    "        ou = np.dot(np.atleast_2d(do * self.dtangent(o)).T, np.atleast_2d(self.x))\n",
    "        \n",
    "        dcs = np.clip(e * o * self.dtangent(self.cs) + dfcs, -6, 6)\n",
    "        \n",
    "        dc = dcs * i\n",
    "    \n",
    "        cu = np.dot(np.atleast_2d(dc * self.dtangent(c)).T, np.atleast_2d(self.x))\n",
    "      \n",
    "        di = dcs * c\n",
    "        \n",
    "        iu = np.dot(np.atleast_2d(di * self.dsigmoid(i)).T, np.atleast_2d(self.x))\n",
    "       \n",
    "        df = dcs * pcs\n",
    "        \n",
    "        fu = np.dot(np.atleast_2d(df * self.dsigmoid(f)).T, np.atleast_2d(self.x))\n",
    "       \n",
    "        dpcs = dcs * f\n",
    "        \n",
    "        dphs = np.dot(dc, self.c)[:self.output] + np.dot(do, self.o)[:self.output] + np.dot(di, self.i)[:self.output] + np.dot(df, self.f)[:self.output] \n",
    "       \n",
    "        return fu, iu, cu, ou, dpcs, dphs\n",
    "            \n",
    "    def update(self, fu, iu, cu, ou):\n",
    "        #Update forget, input, cell, and output gradients\n",
    "        self.Gf = 0.9 * self.Gf + 0.1 * fu**2 \n",
    "        self.Gi = 0.9 * self.Gi + 0.1 * iu**2   \n",
    "        self.Gc = 0.9 * self.Gc + 0.1 * cu**2   \n",
    "        self.Go = 0.9 * self.Go + 0.1 * ou**2   \n",
    "        \n",
    "        #Update our gates using our gradients\n",
    "        self.f -= self.learning_rate/np.sqrt(self.Gf + 1e-8) * fu\n",
    "        self.i -= self.learning_rate/np.sqrt(self.Gi + 1e-8) * iu\n",
    "        self.c -= self.learning_rate/np.sqrt(self.Gc + 1e-8) * cu\n",
    "        self.o -= self.learning_rate/np.sqrt(self.Go + 1e-8) * ou\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Inititialization And Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 1 dimension(s) and the array at index 1 has 2 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[50], line 54\u001B[0m\n\u001B[0;32m     43\u001B[0m \u001B[38;5;66;03m# for i in range(len(x_train)):\u001B[39;00m\n\u001B[0;32m     44\u001B[0m \u001B[38;5;66;03m#     x = x_train[:, i].reshape(-1, 1)\u001B[39;00m\n\u001B[0;32m     45\u001B[0m \u001B[38;5;66;03m#     y=y_train[i]\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     50\u001B[0m \u001B[38;5;66;03m#     RNN.update(learning_rate)\u001B[39;00m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;66;03m#training time!\u001B[39;00m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;28mlen\u001B[39m(train_data)):\n\u001B[0;32m     53\u001B[0m     \u001B[38;5;66;03m#Predict the next word of each word\u001B[39;00m\n\u001B[1;32m---> 54\u001B[0m     \u001B[43mRNN\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforwardProp\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     55\u001B[0m     \u001B[38;5;66;03m#update all our weights using our error\u001B[39;00m\n\u001B[0;32m     56\u001B[0m     error \u001B[38;5;241m=\u001B[39m RNN\u001B[38;5;241m.\u001B[39mbackProp()\n",
      "Cell \u001B[1;32mIn[48], line 54\u001B[0m, in \u001B[0;36mRecurrentNeuralNetwork.forwardProp\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforwardProp\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m     53\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrecurrences\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m---> 54\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mLSTM\u001B[38;5;241m.\u001B[39mx \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhstack\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mha\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     55\u001B[0m         cs, hs, f,inp, c, o \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mLSTM\u001B[38;5;241m.\u001B[39mforwardProp()\n\u001B[0;32m     56\u001B[0m         \u001B[38;5;66;03m#store cell state from the forward propagation\u001B[39;00m\n",
      "File \u001B[1;32m<__array_function__ internals>:200\u001B[0m, in \u001B[0;36mhstack\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "File \u001B[1;32m~\\Downloads\\RNN-LSTM-without-Libraries-master\\RNN-LSTM-without-Libraries-master\\venv\\lib\\site-packages\\numpy\\core\\shape_base.py:368\u001B[0m, in \u001B[0;36mhstack\u001B[1;34m(tup, dtype, casting)\u001B[0m\n\u001B[0;32m    366\u001B[0m \u001B[38;5;66;03m# As a special case, dimension 0 of 1-dimensional arrays is \"horizontal\"\u001B[39;00m\n\u001B[0;32m    367\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m arrs \u001B[38;5;129;01mand\u001B[39;00m arrs[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m--> 368\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_nx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconcatenate\u001B[49m\u001B[43m(\u001B[49m\u001B[43marrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcasting\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcasting\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    369\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    370\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _nx\u001B[38;5;241m.\u001B[39mconcatenate(arrs, \u001B[38;5;241m1\u001B[39m, dtype\u001B[38;5;241m=\u001B[39mdtype, casting\u001B[38;5;241m=\u001B[39mcasting)\n",
      "File \u001B[1;32m<__array_function__ internals>:200\u001B[0m, in \u001B[0;36mconcatenate\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 1 dimension(s) and the array at index 1 has 2 dimension(s)"
     ]
    }
   ],
   "source": [
    "from pandas_datareader.data import DataReader\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "yf.pdr_override()\n",
    "from datetime import datetime\n",
    "tech_list = ['AAPL', 'GOOG', 'MSFT', 'AMZN']\n",
    "\n",
    "tech_list = ['AAPL', 'GOOG', 'MSFT', 'AMZN']\n",
    "\n",
    "end = datetime.now()\n",
    "start = datetime(end.year - 1, end.month, end.day)\n",
    "for stock in tech_list:\n",
    "    globals()[stock] = yf.download(stock, start, end)\n",
    "iterations=1000\n",
    "learningRate=0.1\n",
    "\n",
    "df = DataReader('AAPL', start=datetime(2012,1,1), end=datetime.now())\n",
    "data = df.filter(['Close'])\n",
    "dataset = data.values\n",
    "\n",
    "training_data_len = int(np.ceil( len(dataset) * .95 ))\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaled_data = scaler.fit_transform(dataset)\n",
    "# GET CELLS\n",
    "training_data_len = int(np.ceil( len(dataset) * .95 ))\n",
    "train_data = scaled_data[0:int(training_data_len), :]\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "## 60% train -> 40% test\n",
    "for i in range(60, len(train_data)):\n",
    "    x_train.append(train_data[i-60:i, 0])\n",
    "    y_train.append(train_data[i, 0])\n",
    "\n",
    "x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "# y_train = np.reshape(y_train, (y_train.shape[0], 1, 0))\n",
    "\n",
    "#Initialize the RNN using our hyperparameters and data\n",
    "RNN = RecurrentNeuralNetwork(len(x_train), len(y_train), len(train_data), y_train, learningRate)\n",
    "# for i in range(len(x_train)):\n",
    "#     x = x_train[:, i].reshape(-1, 1)\n",
    "#     y=y_train[i]\n",
    "#     h_prev, c_prev, y_pred = RNN.forwardProp()\n",
    "#     loss = (y_pred - y) ** 2\n",
    "#     grad_output = 2 * (y_pred - y)\n",
    "#     RNN.backward(x, h_prev, c_prev, grad_output)\n",
    "#     RNN.update(learning_rate)\n",
    "#training time!\n",
    "for i in range(1, len(train_data)):\n",
    "    #Predict the next data\n",
    "    RNN.forwardProp()\n",
    "    #update all our weights using our error\n",
    "    error = RNN.backProp()\n",
    "    #For a given error threshold\n",
    "    print(\"Reporting error on iteration \", i, \": \", error)\n",
    "    if error > -10 and error < 10 or i % 10 == 0:\n",
    "        #We provide a seed\n",
    "        seed = np.zeros_like(RNN.x)\n",
    "        maxI = np.argmax(np.random.random(RNN.x.shape))\n",
    "        seed[maxI] = 1\n",
    "        RNN.x = seed\n",
    "        #and predict the upcoming one\n",
    "        output = RNN.sample()\n",
    "        print(output)\n",
    "        #finally, we store it to disk\n",
    "        print(output, data)\n",
    "        print(\"Done Writing\")\n",
    "# print(\"Train/Prediction routine complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
